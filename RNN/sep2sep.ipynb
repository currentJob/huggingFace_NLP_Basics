{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leee2\\AppData\\Local\\Temp\\ipykernel_21548\\622874050.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\leee2\\AppData\\Local\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib3\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded to fra-eng.zip\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def download_zip(url, output_path):\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"ZIP file downloaded to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
    "\n",
    "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "output_path = \"fra-eng.zip\"\n",
    "download_zip(url, output_path)\n",
    "\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, output_path)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 229803\n"
     ]
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "print('전체 샘플의 개수 :',len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58712</th>\n",
       "      <td>It seems warm outside.</td>\n",
       "      <td>Il a l'air de faire bon dehors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13690</th>\n",
       "      <td>We're confused.</td>\n",
       "      <td>Nous sommes perplexes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22627</th>\n",
       "      <td>Spit it out, Tom.</td>\n",
       "      <td>Crache-la, Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51638</th>\n",
       "      <td>That's what I wanted.</td>\n",
       "      <td>C'est ce que je voulais.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31202</th>\n",
       "      <td>You can come home.</td>\n",
       "      <td>Tu peux venir à la maison.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>You go ahead.</td>\n",
       "      <td>Vas-y.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19939</th>\n",
       "      <td>He gave a speech.</td>\n",
       "      <td>Il a donné un discours.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58111</th>\n",
       "      <td>I'm failing at my job.</td>\n",
       "      <td>J'échoue dans mon travail.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51032</th>\n",
       "      <td>Please don't go home.</td>\n",
       "      <td>Ne va pas chez toi, je t'en prie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34537</th>\n",
       "      <td>I'm going to do it.</td>\n",
       "      <td>Je vais le faire.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                tar\n",
       "58712  It seems warm outside.    Il a l'air de faire bon dehors.\n",
       "13690         We're confused.             Nous sommes perplexes.\n",
       "22627       Spit it out, Tom.                    Crache-la, Tom.\n",
       "51638   That's what I wanted.           C'est ce que je voulais.\n",
       "31202      You can come home.         Tu peux venir à la maison.\n",
       "6908            You go ahead.                             Vas-y.\n",
       "19939       He gave a speech.            Il a donné un discours.\n",
       "58111  I'm failing at my job.         J'échoue dans mon travail.\n",
       "51032   Please don't go home.  Ne va pas chez toi, je t'en prie.\n",
       "34537     I'm going to do it.                  Je vais le faire."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:60000] # 6만개만 저장\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21822</th>\n",
       "      <td>It happens a lot.</td>\n",
       "      <td>\\t Cela arrive souvent. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43134</th>\n",
       "      <td>She has few friends.</td>\n",
       "      <td>\\t Elle a peu d'amies. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51262</th>\n",
       "      <td>She told him her age.</td>\n",
       "      <td>\\t Elle lui a dit son âge. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57490</th>\n",
       "      <td>I smiled at everybody.</td>\n",
       "      <td>\\t J'ai souri à tout le monde. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29415</th>\n",
       "      <td>They can't see me.</td>\n",
       "      <td>\\t Elles ne peuvent pas me voir. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26171</th>\n",
       "      <td>He's raking it in.</td>\n",
       "      <td>\\t Il s'en fourre plein les fouilles. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19048</th>\n",
       "      <td>You're so sweet.</td>\n",
       "      <td>\\t Vous êtes si gentils ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23057</th>\n",
       "      <td>They're all dead.</td>\n",
       "      <td>\\t Ils sont tous morts. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15284</th>\n",
       "      <td>I don't need it.</td>\n",
       "      <td>\\t Je n'en ai pas besoin. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22901</th>\n",
       "      <td>The pool is full.</td>\n",
       "      <td>\\t La piscine est pleine. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                       tar\n",
       "21822       It happens a lot.                \\t Cela arrive souvent. \\n\n",
       "43134    She has few friends.                 \\t Elle a peu d'amies. \\n\n",
       "51262   She told him her age.             \\t Elle lui a dit son âge. \\n\n",
       "57490  I smiled at everybody.         \\t J'ai souri à tout le monde. \\n\n",
       "29415      They can't see me.       \\t Elles ne peuvent pas me voir. \\n\n",
       "26171      He's raking it in.  \\t Il s'en fourre plein les fouilles. \\n\n",
       "19048        You're so sweet.              \\t Vous êtes si gentils ! \\n\n",
       "23057       They're all dead.                \\t Ils sont tous morts. \\n\n",
       "15284        I don't need it.              \\t Je n'en ai pas besoin. \\n\n",
       "22901       The pool is full.              \\t La piscine est pleine. \\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 집합 구축\n",
    "src_vocab = set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 문자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 char 집합 : 80\n",
      "target 문장의 char 집합 : 104\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print('source 문장의 char 집합 :',src_vocab_size)\n",
    "print('target 문장의 char 집합 :',tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, 'ï': 77, '’': 78, '€': 79}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '\\xa0': 78, '«': 79, '»': 80, 'À': 81, 'Ç': 82, 'É': 83, 'Ê': 84, 'Ô': 85, 'à': 86, 'â': 87, 'ç': 88, 'è': 89, 'é': 90, 'ê': 91, 'ë': 92, 'î': 93, 'ï': 94, 'ô': 95, 'ù': 96, 'û': 97, 'œ': 98, '\\u2009': 99, '‘': 100, '’': 101, '\\u202f': 102, '‽': 103}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "\n",
    "# 1개의 문장\n",
    "for line in lines.src:\n",
    "  encoded_line = []\n",
    "  # 각 줄에서 1개의 char\n",
    "  for char in line:\n",
    "    # 각 char을 정수로 변환\n",
    "    encoded_line.append(src_to_index[char])\n",
    "  encoder_input.append(encoded_line)\n",
    "print('source 문장의 정수 인코딩 :',encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장의 정수 인코딩 : [[1, 3, 48, 52, 3, 4, 3, 2], [1, 3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [1, 3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [1, 3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [1, 3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    encoded_line.append(tar_to_index[char])\n",
    "  decoder_input.append(encoded_line)\n",
    "print('target 문장의 정수 인코딩 :',decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장 레이블의 정수 인코딩 : [[3, 48, 52, 3, 4, 3, 2], [3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "  timestep = 0\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    if timestep > 0:\n",
    "      encoded_line.append(tar_to_index[char])\n",
    "    timestep = timestep + 1\n",
    "  decoder_target.append(encoded_line)\n",
    "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 최대 길이 : 22\n",
      "target 문장의 최대 길이 : 76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print('source 문장의 최대 길이 :',max_src_len)\n",
    "print('target 문장의 최대 길이 :',max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\leee2\\AppData\\Local\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "\n",
    "# encoder_outputs은 여기서는 불필요\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\leee2\\AppData\\Local\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "\n",
    "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:From c:\\Users\\leee2\\AppData\\Local\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "750/750 [==============================] - 123s 157ms/step - loss: 0.8417 - val_loss: 0.7681\n",
      "Epoch 2/40\n",
      "750/750 [==============================] - 106s 141ms/step - loss: 0.5704 - val_loss: 0.6641\n",
      "Epoch 3/40\n",
      "750/750 [==============================] - 95s 126ms/step - loss: 0.5025 - val_loss: 0.5983\n",
      "Epoch 4/40\n",
      "750/750 [==============================] - 92s 122ms/step - loss: 0.4562 - val_loss: 0.5554\n",
      "Epoch 5/40\n",
      "750/750 [==============================] - 94s 125ms/step - loss: 0.4207 - val_loss: 0.5170\n",
      "Epoch 6/40\n",
      "750/750 [==============================] - 93s 123ms/step - loss: 0.3943 - val_loss: 0.4913\n",
      "Epoch 7/40\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.3739 - val_loss: 0.4728\n",
      "Epoch 8/40\n",
      "750/750 [==============================] - 92s 122ms/step - loss: 0.3576 - val_loss: 0.4545\n",
      "Epoch 9/40\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.3439 - val_loss: 0.4422\n",
      "Epoch 10/40\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.3323 - val_loss: 0.4305\n",
      "Epoch 11/40\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.3222 - val_loss: 0.4203\n",
      "Epoch 12/40\n",
      "750/750 [==============================] - 97s 129ms/step - loss: 0.3131 - val_loss: 0.4109\n",
      "Epoch 13/40\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.3049 - val_loss: 0.4037\n",
      "Epoch 14/40\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2974 - val_loss: 0.3957\n",
      "Epoch 15/40\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.2907 - val_loss: 0.3911\n",
      "Epoch 16/40\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.2846 - val_loss: 0.3871\n",
      "Epoch 17/40\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2790 - val_loss: 0.3820\n",
      "Epoch 18/40\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2737 - val_loss: 0.3770\n",
      "Epoch 19/40\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.2688 - val_loss: 0.3711\n",
      "Epoch 20/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2641 - val_loss: 0.3699\n",
      "Epoch 21/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2599 - val_loss: 0.3677\n",
      "Epoch 22/40\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2557 - val_loss: 0.3635\n",
      "Epoch 23/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2517 - val_loss: 0.3596\n",
      "Epoch 24/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2480 - val_loss: 0.3599\n",
      "Epoch 25/40\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.2446 - val_loss: 0.3557\n",
      "Epoch 26/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2412 - val_loss: 0.3551\n",
      "Epoch 27/40\n",
      "750/750 [==============================] - 88s 117ms/step - loss: 0.2378 - val_loss: 0.3521\n",
      "Epoch 28/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2349 - val_loss: 0.3533\n",
      "Epoch 29/40\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.2318 - val_loss: 0.3504\n",
      "Epoch 30/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2291 - val_loss: 0.3486\n",
      "Epoch 31/40\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2264 - val_loss: 0.3478\n",
      "Epoch 32/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2238 - val_loss: 0.3492\n",
      "Epoch 33/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2212 - val_loss: 0.3472\n",
      "Epoch 34/40\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2187 - val_loss: 0.3440\n",
      "Epoch 35/40\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2164 - val_loss: 0.3451\n",
      "Epoch 36/40\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2142 - val_loss: 0.3441\n",
      "Epoch 37/40\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2119 - val_loss: 0.3437\n",
      "Epoch 38/40\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2097 - val_loss: 0.3438\n",
      "Epoch 39/40\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.2076 - val_loss: 0.3439\n",
      "Epoch 40/40\n",
      "750/750 [==============================] - 88s 117ms/step - loss: 0.2056 - val_loss: 0.3419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x159b44f8790>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
    "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "            len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장: Marche. \n",
      "번역 문장: Va ! \n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장: En route ! \n",
      "번역 문장: Va ! \n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장: Bouge ! \n",
      "번역 문장: Va ! \n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Hi.\n",
      "정답 문장: Salut ! \n",
      "번역 문장: Salut ! \n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Hi.\n",
      "정답 문장: Salut. \n",
      "번역 문장: Salut ! \n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장: Cours ! \n",
      "번역 문장: Prudez-nous. \n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장: Courez ! \n",
      "번역 문장: Prudez-nous. \n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장: Prenez vos jambes à vos cous ! \n",
      "번역 문장: Prudez-nous. \n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장: File ! \n",
      "번역 문장: Prudez-nous. \n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장: Filez ! \n",
      "번역 문장: Prudez-nous. \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1,2,3,4,5,6,7,8,9,10]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
